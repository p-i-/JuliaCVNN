# ğ‘³

topology = [-1, 28*28, 2000, 1000, 500, 100, 50, 20, 1]


const USE_QR = false
const USE_DROPOUT = false
const MAX_EPOCH = 1000

function BATCH_SIZE(epoch)
  trunc(epoch / 5) + 1
end



using Gallium
breakpoint_on_error()

const Ï„,ğ”¦ = 2Ï€, 1.0im
const ğ”¦Ï€, ğ”¦Ï„ = ğ”¦*Ï€, ğ”¦*Ï„

println("start")

typealias â„‚ Complex{Float64}

function Base.angle(zâ‚::â„‚, zâ‚‚::â„‚)::Float64
  Î± = abs( angle(zâ‚‚) - angle(zâ‚) )
  return min(Î±, 2Ï€-Î±)
end

type Layer
  #N::Int            # #neurons
  first::Bool
  last::Bool

  dropout::Float64  # 0.2 means 20% of neurons are dropped/OFF
  âœ”ï¸::Any           # ON/OFF for each neuron

  W::Matrix{â„‚}
  b::Vector{â„‚}

  x::Any
  y::Any
  z::Any

  Î”::Any
  Î´::Any


  function Layer(dropout, Nprev, N)
    âœ”ï¸ = ones(N)
    if Nprev==-1
      new(true,false,dropout,âœ”ï¸)
    else
      W = rand(N, Nprev) .* exp(ğ”¦Ï„*rand(N, Nprev))
      b = rand(N)        .* exp(ğ”¦Ï„*rand(N))
      new(false,false,dropout,âœ”ï¸,W,b)
    end
  end
end

if false
  Juno.breakpoint(@__FILE__, 158)
end

Pkg.add("MNIST")
using MNIST

function main()
  # NETWORK of LAYERS
  Î˜ = Vector{Layer}()

  for i = 2 : length(topology)
    dropout = i==2 ? .2 : .5
    L = Layer( USE_DROPOUT ? dropout : 0.0, topology[i-1:i]... )
    push!(Î˜, L)
  end
  Î˜[end].last = true
  Î˜[end].dropout = 0


  ğ’³, ğ’´ = traindata()
  # ğ’³ = ğ’³[:,1:6000]
  # ğ’´ = ğ’´[  1:6000]

  for epoch = 1 : MAX_EPOCH
    println("Epoch ", epoch)

    batchSize = BATCH_SIZE(epoch)
    nBatches = trunc(Int, length(ğ’´) / batchSize)
    indices = reshape( shuffle(1 : nBatches*batchSize), nBatches, batchSize )

    error = 0.0
    nCorrect = 0

progress( name="batch#" ) do p
    for kBatch = 1 : nBatches

      #print(".")
      progress(p, kBatch/nBatches)

      #Turn units on/off according to dropout
      for L in Î˜
        L.âœ”ï¸ = rand(length(L.âœ”ï¸)) .> L.dropout
        if sum(L.âœ”ï¸) == 0
          L.âœ”ï¸[1] = true    # ensure ONE unit per layer is ON
        end
      end
      Î˜[end].âœ”ï¸ = true      # all units in OUTPUT layer ON

      # Get batch!
      ğ“ = ğ’³[:,indices[kBatch,:]] / 255.0
      ğ“ = ğ’´[  indices[kBatch,:]] / 10.0

      Î˜[1].y = ğ“ .* exp( ğ”¦Ï€ * ğ“ ) .* Î˜[1].âœ”ï¸ # -> upper half of unit circle
      T      =      exp( ğ”¦Ï„ * ğ“ )            # -> unit circle

      #Forward propagate x
      for i = 2 : length(Î˜)
        â—€ï¸L, L = Î˜[i-1:i]
        L.x = â—€ï¸L.y
        L.z = L.W * L.x  .+  L.b  # tried / L.W * L.x * (1-L.dropout) but no luck
        # Activation Function: Don't normalize output layer!
        Ïƒ(z) = L.last ? z : z ./ norm.(z)
        L.y = Ïƒ(L.z) .* L.âœ”ï¸
      end

      L = Î˜[end]

      # Î” represents â€˜network errorâ€™ for each neuron in the layer.
      L.Î” = -L.y + T.';

      âˆ  = angle.( L.y[1,:], T ) # <-- !!! ASSUMING ONLY ONE OUTPUT NEURON
      nCorrect += sum( âˆ  .< Ï€/10 )
      error += mean(âˆ )



      # Back-propagate network error
      #for â—€ï¸L in Î˜[end-1:-1:1]
      for i = length(Î˜) : -1 : 2
        â—€ï¸L, L = Î˜[i-1:i]

        L.Î´ = L.Î” / (sum(â—€ï¸L.âœ”ï¸) + 1)
        if ! â—€ï¸L.first # no Î”, Î´ for INPUT layer
          # For each neuron of prev layer â—€ï¸L, set:
          #    â—€ï¸L.Î” = SUM over each neuron L in this layer of:
          #        L.Î´ / L.weightToâ—€ï¸L
          # Examine on paper:
          #    {the vector L.Î´} * {the matrix of weight reciprocals 1 ./ L.W}
          Î´ = L.Î´ .* L.âœ”ï¸
          wáµ€ = 1 ./ L.W.'

          â—€ï¸L.Î” = wáµ€ * Î´
        end
        #L = â—€ï¸L
      end

      # For each neuron in network, distribute Î´ among weights
      for L in Î˜[2:end]
        N = size(L.x, 1) # #inputs to final layer
        if USE_QR  &&  L.last  &&  batchSize >= N
          # Optimization, see:
          #    "A modified learning algorithm for the multilayer neural network
          #      with multi-valued neurons based on the complex QR decomposition"
          #    Igor Aizenberg, Antonio Luchetta, and Stefano Manetti
          #    Soft Computing, vol. 16, No 4, April 2012, pp. 563-575
          A = hcat(ones(batchSize), L.x.')
          # Have A Î´W = L.Î”, want Î´W
          #   Ax=b  =>  x = A\B  <-- Want this one!
          #   xA=b  =>  x = A/B
          Î”W = A \ L.Î”.'

          ğœ•b = Î”W[1]
          ğœ•W = Î”W[2:end].'
        else
          penalty = L.last ? 1 :  1 ./ norm.(L.z)
          Î´ = penalty .* L.Î´ .* L.âœ”ï¸
          xÌ„áµ€ = L.x'                   # <-- CONJUGATE Transpose!
          ğœ•W = Î´ * xÌ„áµ€ ./ batchSize
          ğœ•b = vec( mean(Î´,2) )
        end

        L.W += ğœ•W
        L.b += ğœ•b
      end

    end #kBatch
end
    @printf " Error: %f, Misclassified: %d \n"  (error/nBatches)/(Ï€/2)  nBatches*batchSize-nCorrect
  end #epoch

  println("done!")
end
main()

#ones(Float32,10,5)*im
#fill(one(Float32)*im, 10, 5)
#fill(1f0im, 10, 5)
#ones(Complex{Float32}, 10, 5)

# Note: To avoid allocation, could do:
#   copy!( L_prev.Î”, result )
#   A .= B ??
#   A[:] = B

# L = Dict{Symbol, Any}()  # Dict{Symbol,Any}[]
# L[:W] = exp(Î¹*Ï„*rand(N))
# L[:b] = exp(Î¹*Ï„*rand())
# L[:N] = N

# Error: 0.334363, Misclassified: 4656 .05     (100 its)
# Error: 0.339458, Misclassified: 4773 0 to 2  (10 its)
# Error: 0.333875, Misclassified: 4669 1       (7 its)
# Error: 0.276041, Misclassified: 4283 0 to 1  (10 its) <-- WINNER
